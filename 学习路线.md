# MiniMind 系统化学习路线（最终版）

本路线融合多份学习计划，以 **AI 系统工程师** 的视角，带你系统地从"能跑通模型"进阶到"能改写算子、优化系统"。整个学习分为两大阶段：前 12 周主线（搭建从训练到部署的闭环）和后 12 周强化（深挖推理性能优化与工程落地）。每周包含明确的周目标、每日任务、以及交付物和验收标准，要求动手实践、记录分析、产出成果。请根据自身进度酌情调整，并坚持记录实验过程和思考，让学习成果可复现、可验证。

**学习原则**：产出驱动、循序渐进；每个阶段都聚焦"读懂代码→跑起来复现→改进验证"的闭环。

**验收方式**：所有改动都需要有统一的基准对比（训练：loss曲线/收敛速度/吞吐/显存；推理：首token延迟（TTFT）/TPS/显存；质量：困惑度PPL等）。

## 阶段一：12 周主线

### 第 1 周：环境配置与基线验证

**周目标**：搭建开发环境，复现 MiniMind 的官方训练与推理。确保掌握实验流程及日志管理。

#### Day 1: 环境搭建

- **操作**：克隆 MiniMind 仓库（`git clone https://github.com/jingyaogong/minimind.git`），创建实验分支；安装依赖（Python、PyTorch、CUDA、常用库，如torchviz, netron, triton, tensorboard, nvitop 等）。
- **分析**：检查 GPU/CPU 可用情况，CUDA 驱动与显存配置；了解 README 中的最小训练/推理命令。
- **产出**：环境依赖安装记录。

#### Day 2: 基线训练复现

- **操作**：运行项目默认训练脚本（例如 `python train.py`），观察是否正常启动；记录训练过程日志（loss 曲线、吞吐、显存峰值）。
- **分析**：确认训练可以正常进行，loss是否在合理范围（即使下降缓慢）；若训练失败，调试依赖或环境问题。
- **产出**：实验记录-1（环境）.md：包含系统配置、环境依赖版本、训练命令、基本运行结果（如初始 loss），以及遇到的问题与解决方案。

#### Day 3: 基线推理测试

- **操作**：运行项目自带的推理脚本（如 `scripts/convert_model.py` 或 `scripts/serve_openai_api.py`），生成一段文本。
- **分析**：记录推理耗时（TTFT、TPS）和显存使用；检查输出文本格式是否正常（即使质量粗糙）。
- **产出**：实验记录-1（环境）.md（继续记录）：推理命令、输出示例、性能数据。

**交付物**：基础环境记录文档 `实验记录-1（环境）.md`，包括硬件/软件配置、运行命令、训练与推理的基本输出（Loss 下降曲线、初始生成示例）。

**验收标准**：成功启动训练并观测到Loss下降；完成一次推理并得到合理格式的输出；所有步骤可复现并有日志可查。

### 第 2 周：静态架构与张量形状推演

**周目标**：深入理解 MiniMind 的模型结构，掌握前向过程中各层张量的形状变化与内存布局。

#### Day 1: 绘制计算图

- **操作**：编写 `debug_graph.py` 脚本，实例化模型 `MiniMindLM`，构造一个小批量 Dummy 输入（如 `input_ids = torch.randint(0, 100, (1, 10))`），执行一次前向并使用 `torchviz.make_dot` 生成计算图 PDF。
- **分析**：在生成的计算图中定位Attention、MLP 等模块，观察注意力中的 Q、K、V 是如何从一个线性层（c_attn）中分离出来的；找出残差连接在图中的长跳线。
- **产出**：`model_graph.pdf`（计算图文件），并在图中标注出 Attention 和 MLP 的模块边界。

#### Day 2: 手推张量形状（Shape Inference）

- **操作**：打开 `model.py`，新建一个表格（Excel 或 MarkDown 表格），列出每一层操作的名称、输入形状、输出形状、是否发生显存拷贝等。逐行分析 forward 函数中各步骤的形状变化。
- **分析**：重点关注 `view`、`reshape`、`transpose` 等操作是否会触发内存复制，检查张量是否连续；在 Attention 计算中，使用 `x.view(B, nh, T, hs)` 等操作时，理解其对内存布局的影响。
- **产出**：一份完整的"MiniMind 张量形状推演表"。

#### Day 3: 张量连续性验证

- **操作**：在模型关键处插入打印（或断言）代码，如 `print(x.is_contiguous())`，验证在执行重塑操作后张量是否连续；查看 PyTorch 是否自动复制内存或报错。
- **分析**：结合 Day2 的表格，验证推导结果的正确性；理解哪些操作会导致内存重排。
- **产出**：`test_shapes.py`：包含几个单测，验证前向过程中张量的输入输出维度不变性，以及 KV Cache 开关下输出一致性（相同随机种子下）。

#### Day 4: RoPE 旋转编码底层对齐

- **操作**：定位 `model.py` 中 RoPE 实现部分，打印 RoPE 计算前后 `x.stride()`；观察 RoPE 前后张量的内存步长。
- **分析**：理解 RoPE 是如何将向量两两配对（通过 reshape 将最后一维拆成 `[..., head_dim/2, 2]`）；评估这种 reshape 是否破坏内存连续性，以及为什么需要这样做。
- **产出**：一段注释或报告，解释 RoPE 所使用的 reshape 过程和对内存布局的影响。

#### Day 5: 参数量与理论 FLOPs

- **操作**：编写脚本统计 MiniMind 每层的参数量（或使用 PyTorch 内置统计）；手算一次前向传播的理论 FLOPs 数量，并编写测试代码验证与实际运行时间的对比。
- **分析**：计算模型的 MFU（Model FLOPs Utilization）指标，评估模型的计算利用率。
- **产出**：参数量统计表和 FLOPs 计算说明文档；如果有测试代码，则包括具体输出对比结果。

**交付物**：`model_graph.pdf`（标注图），张量形状推演表，`test_shapes.py`，RoPE 分析说明，参数统计报告等。

**验收标准**：能够在白板上完整描述模型前向流程中的每一步张量形状及主要计算量来源；所有形状测试通过，RoPE 机制解释正确。

### 第 3 周：数据预处理与性能分析入门

**周目标**：理解数据管线瓶颈和推理算子性能分布，发现性能热点。

#### Day 1: 数据流程可视化

- **操作**：阅读 `dataset/` 目录下的数据处理脚本，绘制"原始语料→分词（Tokenizer）→Pack→DataLoader"流程图。
- **分析**：了解数据打包（pack）对 batch 形状的影响及副作用（如序列跨样本拼接的注意力和位置编码问题）。
- **产出**：数据流程图文档（如 Markdown + 图），标注每个阶段的输入输出形状。

#### Day 2: DataLoader 参数实验

- **操作**：做 1 个小实验比较"有 Pack vs 无 Pack"情况下的训练效率和模型损失变化；尝试不同 `max_length` 对训练稳定性的影响。
- **分析**：观察批处理方式对训练速度（tokens/s）和收敛的影响，分析 Pack 带来的内存和计算效益或问题。
- **产出**：对比实验的曲线图和结论说明，记录不同设置下的吞吐、loss 等。

#### Day 3: DataLoader 瓶颈定位

- **操作**：使用 `py-spy record -o profile.svg --pid <PID>` 等工具，对训练过程进行采样分析，生成火焰图；分析 CPU 与 GPU 的工作时间占比。
- **分析**：检查是否存在 CPU Tokenize 阶段过重；打开火焰图，找出主要的时间消耗函数，关注 `__getitem__` 中的开销。
- **产出**：CPU 火焰图截图及分析报告。

#### Day 4: DataLoader 参数调优

- **操作**：实验设置 `num_workers=0` vs `num_workers>0`，并开启 `pin_memory=True/False`，使用 `watch -n 0.1 nvidia-smi` 监控 GPU 利用率。
- **分析**：比较不同参数组合下 GPU 利用率与训练速度的变化，理解多进程读取和内存锁页技术（pin_memory）带来的加速原理。
- **产出**：GPU 利用率对比图表及优化建议。

#### Day 5: PyTorch Profiler 初探

- **操作**：在一次推理或小规模训练循环中使用 `torch.profiler.profile` 抓取 trace（例如 `wait=1, warmup=1, active=3` 导出 `trace.json`）。
- **分析**：使用 Chrome Tracing（`chrome://tracing`）打开 `trace.json`，观察 GPU Kernel 间是否有空闲时隙（Gap）、识别 Top-k 耗时算子（Gemm、Softmax 等）。
- **产出**：Profiling 截图（Top 5 耗时算子）和简要分析说明。

**交付物**：数据流程图、对比实验报告、火焰图与GPU利用率图表、Profiler TopK 截图等。

**验收标准**：能够解释数据批处理方式对训练速度和收敛的影响；识别数据加载的瓶颈（CPU Tokenize 或数据拷贝）；列出主要耗时算子并理解是否存在 GPU 空闲。

### 第 4 周：训练系统与数值稳定

**周目标**：掌握混合精度训练的数值稳定性问题，以及训练效率的监控和优化。

#### Day 1: 训练过程监控

- **操作**：在 `trainer.py` 中添加监控代码，记录训练过程中的 **吞吐率**（tokens/s、step time）和 **显存占用**（各阶段峰值）。
- **分析**：运行一次训练，记录基准吞吐和显存，找出是前向、反向还是更新阶段最耗时或显存最高。
- **产出**：基础训练监控报告（吞吐率、显存曲线）。

#### Day 2: FP32 vs AMP 对比

- **操作**：分别在 FP32 模式和 AMP 模式下运行训练（可使用 `torch.cuda.amp`），记录速度、显存占用和最终模型稳定性（是否有 NaN）。
- **分析**：比较两者的训练速度提升率、显存节省幅度以及收敛差异，熟悉 AMP 的使用流程。
- **产出**：对比实验结果（表格或曲线），记录 AMP 下 Scale Factor 的初始值。

#### Day 3: 梯度累积与超参

- **操作**：开启梯度累积（模拟更大有效 Batch）与不使用累积，两者在相同等效 batch size 下运行训练。
- **分析**：比较吞吐率和收敛情况，确认梯度累积对显存和速度的影响。
- **产出**：累积与否对比报告。

#### Day 4: 人为制造数值不稳定

- **操作**：在模型前向中故意放大输出（如乘以100）或将学习率调大 100 倍，触发梯度爆炸/NaN。
- **分析**：在 `trainer.py` 中打印各层梯度范数（使用 `torch.nn.utils.clip_grad_norm_`），观察梯度变为 Inf/NaN 的时刻；使用 `torch.autograd.set_detect_anomaly(True)` 定位异常产生位置。
- **产出**：记录破坏前后几个 training step 的梯度范数和损失值。

#### Day 5: AMP Scale 调整机制

- **操作**：继续在 AMP 模式下训练，当出现 Inf 时，观察 `GradScaler.get_scale()` 的变化（Scale Factor 会自动减半）。
- **分析**：了解混合精度下 Scale 调整的原理（遇到溢出时降低 Scale），并确保训练流程正确跳过溢出步。
- **产出**：Loss Scale 动态调整日志，包含每次调整的 Scale 数值。

**交付物**：训练系统分析.md（包括吞吐与显存记录、FP32 vs AMP 实验结论、梯度爆炸定位记录、Scale 调整日志等）。

**验收标准**：能够在 30 分钟内定位训练慢、NaN 或显存超限的原因；理解并记录下混合精度训练中 NaN 产生及 Scale 调整机制。

### 第 5 周：Triton 算子开发入门

**周目标**：使用 OpenAI Triton 开发自定义 CUDA Kernel，提升底层算子性能（例如 Norm 操作）。

#### Day 1: Triton 入门教程

- **操作**：阅读 Triton 官方教程（如 `02-fused-softmax`），理解 `@triton.jit` 的基本用法；练习使用 `tl.load` 和 `tl.store` 从显存读写数据。
- **分析**：了解 Triton 如何将批量张量计算映射到 GPU 上，以及 register 和并行概念。
- **产出**：Triton 教程学习笔记。

#### Day 2: 自定义 Kernel 准备

- **操作**：安装 Triton（`pip install triton`），阅读 PyTorch Triton 接口（`torch.ops.triton`）；准备将要优化的算子（如 RMSNorm）。
- **分析**：定位 RMSNorm 原生实现（涉及 `pow`, `mean`, `rsqrt`, `mul` 等操作），评估其性能瓶颈（多次显存读写）。
- **产出**：算子性能分析报告。

#### Day 3: 编写 Triton RMSNorm Kernel

- **操作**：创建 `rms_norm_triton.py`，使用 Triton 编写一个单行数据的 RMSNorm Kernel：先 `tl.load` 一行数据到寄存器，计算平方和、均方根并应用归一化，最后 `tl.store` 回显存。
- **分析**：确保理解每一步骤的内存访问；考虑使用 `BLOCK_SIZE` 来划分线程块。
- **产出**：初步的 Triton RMSNorm 代码。

#### Day 4: Triton 算子测试

- **操作**：在 PyTorch 中封装 `torch.nn.Module` 调用 Triton Kernel，编写测试脚本；使用 `torch.allclose` 对比 Triton 实现与原生 RMSNorm 在各种输入下的数值一致性。
- **分析**：验证误差是否在容许范围（<1e-5）；检查不同 Batch, Seq 长度的正确性。
- **产出**：测试代码和结果日志。

#### Day 5: Triton 算子优化

- **操作**：优化 Kernel 性能，比如使用向量化（`tl.load` 连续内存）、减少 Python 循环；调整 `BLOCK_SIZE` 和 grid 配置确保利用率。
- **分析**：用 Triton 自带的 `triton.testing.do_bench` 简单对比小规模下速度，确认性能提升。
- **产出**：完整可复用的 `TritonRMSNorm` 类（含文档）和基准结果。

**交付物**：`TritonRMSNorm.py`（实现与封装）、测试报告（数值误差对比）、初步性能对比结果图。

**验收标准**：Triton 实现与原生实现误差 < 1e-5；Kernel 能运行并明显减少显存读写步骤。

### 第 6 周：算子集成与性能对比

**周目标**：将自定义 Triton 算子集成到 MiniMind 模型中，并进行性能基准测试；尝试算子融合（如 `torch.compile`）以进一步优化。

#### Day 1: Benchmark 脚本编写

- **操作**：编写基准测试脚本，使用 `triton.testing.do_bench` 或简单循环，测试不同输入尺寸下原生 RMSNorm 与 `TritonRMSNorm` 的执行时间。
- **分析**：记录在较大输入尺寸下的加速比，预期显存 IO 降低带来的 20%-50% 加速。
- **产出**：基准结果数据（表格或 CSV）。

#### Day 2: 算子替换集成

- **操作**：修改 MiniMind 的 `model.py`，在模型初始化时将原生 RMSNorm 替换为 `TritonRMSNorm`；确保模型前向调用生效。
- **分析**：检查模型可训练性，做一次前向测试确认输出正常。
- **产出**：修改后的模型代码片段（或新文件）。

#### Day 3: Triton 加速测试

- **操作**：在相同硬件上比较替换前后的训练/推理性能，重点记录吞吐（tokens/s）和显存峰值。
- **分析**：绘制柱状图对比，验证 Triton 算子带来的性能提升；总结优化效果。
- **产出**：Benchmark 对比图表。

#### Day 4: Torch.compile 算子融合

- **操作**：使用 PyTorch 2.0+ 提供的 `torch.compile` 编译 MiniMind 模型。
- **分析**：对比编译前后模型的推理延迟和使用 `nsys` 或 `torch.profiler` 的时间线图，找出哪些小算子被融合掉了。
- **产出**：nsys 时间线截屏或 Profiler 报告片段，标注被融合的 kernel。

#### Day 5: 集成与验证

- **操作**：将 Triton 算子和编译优化集成进主训练流程，确保可以端到端训练。
- **分析**：综合评估算子替换与融合带来的整体性能改进，更新实验记录。
- **产出**：最终的模型代码（包含 Triton 算子）和详细性能报告。

**交付物**：包含 Triton 算子实现的修改版 `model.py`，性能基准图表，融合前后 Timeline 报告。

**验收标准**：Triton 版本在大输入下显著加速（≥20%）；能够指认 nsys 报告中被融合的算子；模型功能不变且训练稳定。

### 第 7 周：KV Cache 优化与内存管理

**周目标**：优化推理阶段 KV Cache 的内存使用，减少显存碎片和重复申请，以提升推理效率。

#### Day 1: KV Cache 预分配思路

- **操作**：在推理代码中预分配一个足够大的缓存张量，比如 `cache = torch.zeros(batch, max_seq_len, num_heads, head_dim, device='cuda')`，并维护一个当前写入位置指针。
- **分析**：原有实现每步 `torch.cat` 新 KV 到列表会导致重复申请，预分配可以避免这个开销。
- **产出**：预分配 KV Cache 的代码草案。

#### Day 2: 实现预分配缓存

- **操作**：修改 MiniMind 的生成脚本，将每一步生成的 K/V 写入预分配张量（例如：`cache[:, pos, ...] = new_kv`），而不是使用列表 cat。
- **分析**：确保写入逻辑正确，前向推理结果与原实现一致。
- **产出**：更新后的推理脚本代码。

#### Day 3: 性能对比测试

- **操作**：分别运行原始 cat 模式和预分配模式，生成固定长度（如1000 tokens）的文本；记录两者的 TPS（每秒 Token 数）。
- **分析**：绘制对比图，量化改动后的性能提升比例。
- **产出**：TPS 对比数据和柱状图。

#### Day 4: 显存碎片分析

- **操作**：在运行中记录每一步显存分配情况（如使用前面 Week2 自定义的 `@track_memory` 装饰器），比较两种模式下显存增长曲线。
- **分析**：确认预分配模式有效降低了显存碎片和峰值增长速度。
- **产出**：显存使用曲线图。

#### Day 5: 总结与验证

- **操作**：综合分析性能与显存数据，对预分配策略的改进点进行文档总结。
- **产出**：KV Cache 优化报告，说明实现过程与效果。

**交付物**：修改后的推理脚本、对比测试结果（TPS 图表、显存曲线）、优化报告。

**验收标准**：预分配模式下 TPS 明显提升（常见可达 10%-30% 加速）；显存增长曲线更加平滑，验证减少了重复分配。

### 第 8 周：模型量化与导出

**周目标**：了解模型量化的影响，并掌握将模型导出为 ONNX 等格式的过程，为后续部署做准备。

#### Day 1: 伪量化实验

- **操作**：在 `model.py` 中线性层前加入简单的量化模拟，例如：`w_int8 = torch.round(w_fp16 / scale)`，以及反量化逻辑，将权重截断为低精度。
- **分析**：在不修改其他部分的情况下，用量化后的权重推理，观察困惑度（PPL）和输出文本质量的变化。
- **产出**：小型实验记录（量化 scale 变化及其影响）。

#### Day 2: 真实量化工具体验

- **操作**：尝试使用现有工具（如 bitsandbytes 的 8-bit 或 GPTQ 进行 4-bit 量化）对模型参数进行量化。
- **分析**：比较量化后与原模型在验证集或样例上的 PPL 差异；记录量化过程的步骤和问题。
- **产出**：量化结果对比表（FP16 vs INT8/INT4）。

#### Day 3: 导出 ONNX 模型

- **操作**：使用 `torch.onnx.export` 将训练好的 MiniMind 导出为 `.onnx` 文件，指定合适的输入形状和动态轴。
- **分析**：使用 Netron 等工具打开 `.onnx` 文件，观察计算图结构；注意 Python 中的控制流（if/else）如何被展开为静态图。
- **产出**：`minimind.onnx` 文件和 Netron 截图。

#### Day 4: ONNX 推理验证

- **操作**：使用 ONNX Runtime 或其他后端载入 `minimind.onnx`，运行一次推理，验证输出与 PyTorch 原生一致性（或在容差内）。
- **分析**：确保模型成功转换，无算子缺失或形状错误。
- **产出**：ONNX 推理测试脚本与结果对比。

#### Day 5: 模型精度评估

- **操作**：对比量化后（使用 bitsandbytes、gptq）与导出模型在小型测试集上的输出差异。
- **分析**：分析量化带来的精度损失、ONNX 导出后的变更；准备模型压缩的对比报告。
- **产出**：量化与导出分析.md，包含 PPL、输出示例和性能数据。

**交付物**：`minimind.onnx` 文件，Netron 可视化截图，量化与导出对比报告。

**验收标准**：量化后模型仍能生成可读文本（接受一定 PPL 上升）；ONNX 模型正确导出且前向运行正常；报告中清楚说明了量化/导出对性能和精度的影响。

## 阶段二：12 周强化（每两周为一组）

### 第 13-14 周：推理性能剖析

**组目标**：使用 Profiler 和 Nsight Systems 等工具深入剖析推理过程中的瓶颈，为后续优化指明方向。

#### Week13 Day1: Torch Profiler 推理分析

- **操作**：对推理过程分别进行 Prefill 阶段和 Decode 阶段的单独 profiling：使用 `torch.profiler` 分别抓取两部分的 trace。
- **分析**：合并 Profiler 数据，找出 Top 5 耗时算子（通常为 attention、softmax、matmul 等）；观察 GPU kernel 间隙。
- **产出**：Profiler Trace 报告与 TopK 列表。

#### Week13 Day2: Nsight Systems 流程跟踪

- **操作**：使用 `nsys profile` 抓取完整推理会话的 Timeline（包括 CUDA kernel 和 CPU 调度）。
- **分析**：在 Nsight timeline 中检查 GPU 是否存在空闲时间（Launch Bound vs Memory Bound）；查看数据拷贝（H2D/D2H）与计算是否重叠。
- **产出**：Nsight Timeline 截图和分析笔记。

#### Week13 Day3: 并行流与数据传输

- **操作**：分析 Nsight 报告中的多个 CUDA stream 行为，观察主流和辅流之间的并行情况；检查是否实现了数据传输与计算重叠。
- **分析**：确认是否存在效率低下的串行数据传输；如有，考虑使用异步拷贝或双缓冲优化。
- **产出**：并行流分析文档。

#### Week13 Day4: 专业报告撰写

- **操作**：整理 Profiler 和 Nsight 的发现，形成一份《MiniMind 推理性能剖析报告》。
- **分析**：报告中应包含 Timeline 截图、TopK 算子列表、瓶颈分析（计算受限 vs 内存受限），并提出初步优化建议。
- **产出**：性能剖析报告文档。

#### Week13 Day5: 代码级细节检查

- **操作**：针对发现的瓶颈算子（如过多的小 kernel launch 或大规模数据复制），检查 MiniMind 源码中相关实现是否可优化（例如合并 kernel，减少不必要的张量操作）。
- **分析**：记录潜在改进点，为第 21-22 周的改造做准备。
- **产出**：优化建议清单。

**交付物**：`benchmarks/profile_report.md`，包括 Profiler 和 Nsight 截图、分析结论和优化方向建议。

**验收标准**：能够明确指出推理过程中的主要瓶颈（计算 vs 内存 vs Launch），并给出量化数据和可行的改进思路。

### 第 15-16 周：长上下文与KV Cache优化

**组目标**：验证长上下文场景下的性能瓶颈，深入理解 KV Cache 布局与显存消耗，对模型在实际场景中的表现做压测。

#### Week15 Day1: 长上下文内存测试

- **操作**：固定模型和输入，逐渐增加 Prefill 长度（如从 128 到 2048），记录每种长度下的显存峰值和 Prefill 延迟（首token延迟 TTFT）。
- **分析**：绘制显存 vs 上下文长度的曲线，找出 O(T) 增长的位置；解释显存增长的主要来源。
- **产出**：显存与 TTFT 曲线图。

#### Week15 Day2: 长生成吞吐测试

- **操作**：固定 prompt 长度，设置不同的 `max_new_tokens`（如 100、500、1000），记录 Decode 阶段的整体吞吐（TPS）。
- **分析**：绘制 TPS vs 生成长度的曲线，分析生成长文本时主要受限于哪些因素（如 KV concatenation）。
- **产出**：TPS vs 长度曲线图。

#### Week15 Day3: GQA/MQA 布局对比

- **操作**：测试 MiniMind 在 GQA（Grouped QKV）和 MQA（Multi-Query Attention）两种 KV 布局下的推理性能与显存使用。
- **分析**：比较两种模式下长上下文时的优劣，解释原因。
- **产出**：GQA vs MQA 性能对比报告。

#### Week15 Day4: KV Cache 物理结构图

- **操作**：绘制 MiniMind 使用 Prefill + Decode 时的 KV Cache 物理内存结构图示：按层、按头、按序列组织的逻辑视图。
- **分析**：说明当前 KV Cache 实现如何导致显存增长；对比理论上的分页（PagedAttention）方式。
- **产出**：KV Cache 内存逻辑图。

#### Week15 Day5: 长序列总结

- **操作**：综合以上实验结果，撰写 `long_context.md` 报告。
- **分析**：总结在不同上下文长度下性能和显存的变化，提出 KV Cache 优化的思路（如分块、复用）。
- **产出**：长上下文压测报告。

**交付物**：`benchmarks/long_context.md`，包含显存曲线、TPS 曲线、GQA/MQA 比较和结论图示。

**验收标准**：清楚展示长上下文下显存和性能如何随序列长度变化；解释影响性能的主因并提出可能的优化方案。

### 第 17-18 周：模型量化与格式转化

**组目标**：掌握模型精度压缩技术和多种模型格式的转换，完成端侧部署前的准备和对比测试。

#### Week17 Day1: 准备量化工具

- **操作**：使用 bitsandbytes 或 GPTQ 对 MiniMind 模型进行 8-bit/4-bit 量化转换，生成量化后的模型参数。
- **分析**：记录量化过程参数（对称/非对称量化），观察压缩后模型大小。
- **产出**：量化后模型文件（如 `8bit.pt` 或 `4bit.pt`）。

#### Week17 Day2: 量化性能对比

- **操作**：在小型验证集或任务集上，比较原始 FP16 模型与量化模型的性能：评估 PPL（困惑度）、生成质量和推理速度。
- **分析**：记录精度损失幅度和加速率，分析离群值对量化精度的影响。
- **产出**：PPL 和速度对比表格及示例输出。

#### Week17 Day3: llama.cpp 转换

- **操作**：使用 `convert_hf_to_gguf.py` 脚本将模型转换为 llama.cpp 可用的 GGUF 格式，确保词表等兼容性。
- **分析**：尝试在本地 CPU（或 Low-setup GPU）上使用 llama.cpp 运行量化模型，测试推理功能。
- **产出**：转换脚本 `export_gguf.sh` 及运行日志。

#### Week17 Day4: 端侧部署（MNN）

- **操作**：参考 README，使用 MNN 框架导出模型（4-bit HQQ 量化）并部署到非 NVIDIA 设备（如手机仿真器）。
- **分析**：测试模型在端侧的推理性能（延迟、吞吐）和资源占用（内存、CPU）。
- **产出**：端侧部署脚本（`export_mnn.sh`）和运行结果报告。

#### Week17 Day5: 量化报告撰写

- **操作**：整理量化前后模型在精度、推理速度、模型体积上的对比数据，形成 `quantization.md`。
- **产出**：量化对比报告文档。

**交付物**：`benchmarks/quantization.md`（包括 PPL 对比、性能测试截图）、导出脚本（如 `export_gguf.sh`、`export_mnn.sh`）。

**验收标准**：量化前后模型在 PPL 和 TPS 上的差异清晰量化；成功在目标平台（CPU/GPU/移动端）加载运行量化模型，文档齐全。

### 第 19-20 周：高效推理框架接入

**组目标**：学习使用高级推理框架（如 vLLM）进行高并发推理，评估其相对于原生实现的性能优势。

#### Week19 Day1: vLLM 服务化启动

- **操作**：使用 MiniMind README 提供的示例命令启动 vLLM 服务，例如：
  ```bash
  vllm serve ./MiniMind --model-impl transformers --served-model-name "minimind" --port 8998
  ```
- **分析**：确保服务启动成功，能够接收 OpenAI API 兼容请求（使用 openai-serve 格式）。
- **产出**：vLLM 服务端运行日志。

#### Week19 Day2: 并发压测搭建

- **操作**：使用压测工具（Locust、wrk 或 `scripts/bench_infer.py`）模拟多并发环境（如 2/4/8 并发请求），发送推理请求并收集数据。
- **分析**：记录在不同并发等级下的吞吐（TPOT：每输出 Token 时间）和延迟分位数（p50、p95）。
- **产出**：压测脚本及吞吐/延迟数据表。

#### Week19 Day3: 对比分析

- **操作**：将 vLLM 环境下的并发测试结果与原生 FastAPI 服务或单进程推理结果进行对比。
- **分析**：评估并行处理带来的性能提升或瓶颈；验证是否实现了请求的批处理。
- **产出**：并发性能对比报告。

#### Week19 Day4: 文档与示例完善

- **操作**：根据压测经验，完善 vLLM 接入的操作文档（如使用说明、参数配置建议），并将实验代码纳入项目 `benchmarks/` 目录。
- **产出**：`vllm_serving.md` 文档。

#### Week19 Day5: 环境稳定性测试

- **操作**：尝试在本地多卡或不同模型大小下重复测试，观察是否稳定；捕获异常和日志。
- **分析**：确认模型在高并发下无崩溃；记录日志作为报告附录。
- **产出**：补充实验日志与最终报告。

**交付物**：`benchmarks/vllm_serving.md`（并发吞吐与延迟分析）、压测脚本、服务化相关文档。

**验收标准**：成功搭建并测试 vLLM 服务，数据清晰展示并发并行下的吞吐提升；服务稳定运行，能够复现并行推理结果。

### 第 21-22 周：优化改造与 PR

**组目标**：基于前期分析选择一个可量化的优化点或工具链完善项进行改造，并形成可复现的实验报告或开源贡献。

#### Week21 Day1: 确定改造方向

- **操作**：从之前分析出的瓶颈中选择一个切入点，例如：降低 KV Cache 显存、减少 kernel launch、改进持续批处理、整理脚本文档等。
- **分析**：明确该改造的衡量指标（TPS 提升、显存降低、PPL 改进等）和预期幅度。
- **产出**：优化计划草案。

#### Week21 Day2-3: 实施改造

- **操作**：根据方案编写代码或文档：如调整模型超参、开启/关闭 `torch.compile`、优化前后处理代码、添加流水线批处理逻辑、完善文档和脚本等。
- **分析**：在改造过程保持记录，及时测试每个子改动的影响；确保改动可开关复现。
- **产出**：改造后的代码/配置文件。

#### Week21 Day4: Ablation 测试

- **操作**：对比改造前后的性能指标：训练阶段的 loss/吞吐、推理阶段的 TTFT/TPS、显存峰值等。
- **分析**：绘制对比曲线或表格，定量评估改造效果；检查质量影响（PPL、样本输出）。
- **产出**：AB testing 报告。

#### Week21 Day5: 开源贡献准备

- **操作**：将改造结果整理成实验报告，准备提交 PR 或 Issue：补充说明问题背景、改动内容、实验数据。
- **分析**：确保报告中包含 Profile 证据、Benchmark 对比和复现步骤；代码满足项目格式要求。
- **产出**：实验记录文档和 PR/Issue 草稿。

**交付物**：实验记录-改造.md（说明改动点、效果对比）、可复现脚本或 PR 代码。

**验收标准**：改造带来可量化提升（如 TPS 提高 ≥5%、显存降低 ≥10% 或更稳定的训练）；提供完整的性能对比数据和复现命令；若是 PR，应符合项目标准并附有实验依据。

### 第 23-24 周：开源贡献与总结

**组目标**：将学习成果与项目闭环，通过实际的 Issue/PR 贡献给社区，总结完整的学习闭环。

#### Week23 Day1: 提出 Issue

- **操作**：复现并验证一个可改进的问题或缺陷，按照社区要求填写 Issue 模板，提供复现步骤、日志、环境等信息。
- **产出**：MiniMind 仓库 issue 页面截图或链接。

#### Week23 Day2-3: 提交修复或功能改进 PR

- **操作**：根据 Issue 编写修复代码或功能完善（文档、脚本、单元测试等），在项目中提交 PR。
- **分析**：确保 PR 描述清晰、变动小且目标明确；附上对应的测试结果。
- **产出**：PR 链接和变更说明。

#### Week23 Day4: 社区复盘与总结

- **操作**：整理本次学习路线的所有产出（报告、代码、数据），撰写总结文档。
- **产出**：学习总结或博客草稿，涵盖主要收获和未来工作建议。

#### Week23 Day5: 毕业设计准备

- **操作**：整理论文或项目报告的最终骨架和内容概要，准备展示用的材料（PPT、视频或 Demo）。
- **产出**：毕业设计提纲。

**交付物**：至少 1 个 GitHub Issue、1 个 PR（即使是文档完善也可）；完整的项目复现仓库，包括 README、实验数据和分析报告。

**验收标准**：Issue/PR 被接受并合并（或至少通过 review）；可以用标准化方式（Markdown、表格、图表）全面展示改进效果；所交付材料满足毕业设计要求。

## 毕业设计与最终输出

完成以上学习后，建议最终输出以下成果作为你的核心亮点和毕业项目内容：

1. **深度性能报告**：基于 Nsight Systems 或 torch.profiler 的全面性能剖析报告，展示如何识别模型瓶颈并通过算子优化或缓存优化提升吞吐率。
2. **自定义高性能算子**：基于 Triton 实现的高性能算子（如 Norm、Attention Kernel）代码，并集成到模型中进行验证。
3. **完整的压测数据**：包括不同并发数、不同量化精度下的推理延迟/吞吐对比报告（如 Locust 压测数据）。
4. **MiniMind 复现仓库**：包含所有实验代码、脚本、注释和数据的仓库，配备清晰的 README、结构化的 `benchmarks/` 和 `experiments/` 目录。
5. **开源贡献记录**：提交的 Issue 和 PR 链接，显示你对 MiniMind 项目的贡献与改进。

这些成果可作为简历中的实践项目展示，体现你在底层优化和系统工程方面的实战能力。

## 附录：工具链准备、资料推荐与实验模板

### 必备工具：

- **代码阅读**：VS Code（或类似 IDE）+ Sourcetrail 插件，用于生成调用图和类关系图。
- **性能分析**：PyTorch 内置 `torch.profiler`；NVIDIA Nsight Systems (nsys)；Python 的 py-spy（CPU 火焰图）。
- **调试工具**：gdb（C++ 调试）、cuda-gdb（GPU 调试）。
- **算子开发**：OpenAI Triton（Python 风格编写 CUDA Kernel）。
- **其他辅助**：torchviz（计算图可视化）、netron（ONNX 模型可视化）、tensorboard（训练曲线可视化）、nvitop（实时监控 GPU 利用率）。
- **压测工具**：Locust、wrk、自定义 `bench_infer.py` 等。
- **量化工具**：bitsandbytes、GPTQ 库；LLaMA.cpp、MNN 等部署框架。

### 推荐资料：

- MiniMind 官方仓库和 README（提供模型结构、格式转换、推理工具链示例）。
- Triton 官方文档：并行编程与自定义 GPU Kernel 指南。
- PyTorch Profiler 文档 和 Nsight Systems 用户指南。
- CUDA 编程基础、GPU 内存层次结构等 GPU 编程资料；以及 NVIDIA AMP 和混合精度训练教程。
- LLAMA.cpp、MNN、Ollama 等推理框架的官方示例和量化指南。

### 实验记录模板建议：

使用 Markdown 格式记录实验（如 `experiments/` 目录），并用表格整理超参与指标。示例模板：

| 实验编号 | 修改点／超参 | Speed (tokens/s) | PPL | 显存峰值 | 备注 |
|---------|------------|-----------------|-----|---------|------|
| Exp-01 | Base (FP16, bs=8) | 1234 | 20.5 | 6.7GB | 基线 |
| Exp-02 | +Triton RMSNorm | 1460 | 20.5 | 6.5GB | 加速 ~18% |
| Exp-03 | +AMP（Mixed Precision） | 2100 | 20.5 | 4.0GB | 加速~70%，显存减小 |

同时记录环境信息（GPU 型号、显存、CUDA 版本）、命令行和观测日志，保持实验结果可复现。每次实验结束及时总结结论，形成报告文档。
