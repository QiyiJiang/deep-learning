# æ¨¡å‹æ¶æ„ä¼˜åŒ–æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•å°†å½“å‰ DIY æ¨¡å‹æ¶æ„ä¼˜åŒ–ä¸ºä¸ MiniMind2-Smallï¼ˆ26M å‚æ•°ï¼‰å¯¹é½ï¼Œè§£å†³é¢„è®­ç»ƒåæ¨¡å‹å‡ºç°"repeater"ï¼ˆé‡å¤ç”Ÿæˆï¼‰é—®é¢˜ã€‚

**å‚è€ƒé…ç½®**ï¼šMiniMind2-Smallï¼ˆæ¥è‡ª README.mdï¼‰
- å‚æ•°é‡ï¼š26M
- vocab_size: 6400
- rope_theta: 1e6
- n_layers: 8
- d_model: 512
- kv_heads: 2
- q_heads: 8

---

## ğŸ“Š å½“å‰é…ç½® vs ç›®æ ‡é…ç½®å¯¹æ¯”

| é…ç½®é¡¹ | å½“å‰å€¼ | ç›®æ ‡å€¼ï¼ˆMiniMind2-Smallï¼‰ | çŠ¶æ€ |
|--------|--------|---------------------------|------|
| `vocab_size` | 6400 | 6400 | âœ… å·²åŒ¹é… |
| `hidden_size` (d_model) | 512 | 512 | âœ… å·²åŒ¹é… |
| `num_layers` (n_layers) | **23** | **8** | âŒ **éœ€è°ƒæ•´** |
| `num_heads` (q_heads) | 8 | 8 | âœ… å·²åŒ¹é… |
| `num_key_value_heads` (kv_heads) | **ç¼ºå¤±** | **2** | âŒ **éœ€æ–°å¢** |
| `intermediate_size` | **2048 (ç¡¬ç¼–ç )** | **1408 (è‡ªåŠ¨è®¡ç®—)** | âŒ **éœ€è°ƒæ•´** |
| `rope_base` (rope_theta) | 1e6 | 1e6 | âœ… å·²åŒ¹é… |
| `dropout` | 0.1 | 0.0 | âš ï¸ å¯ä¿ç•™ï¼ˆè®­ç»ƒç¨³å®šæ€§ï¼‰ |
| `hidden_act` | **GELU** | **SiLU** | âŒ **éœ€è°ƒæ•´** |
| **GQA å®ç°** | **æœªå®ç°** | **å·²å®ç°** | âŒ **éœ€æ–°å¢** |

---

## ğŸ”´ ä¼˜å…ˆçº§ 1ï¼šå®ç° GQA (Grouped Query Attention)

### é—®é¢˜æè¿°

å½“å‰æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„ Multi-Head Attentionï¼ˆMHAï¼‰ï¼Œæ‰€æœ‰ Qã€Kã€V éƒ½æ˜¯ 8 ä¸ªå¤´ã€‚è€Œ MiniMind2-Small ä½¿ç”¨ **GQA (Grouped Query Attention)**ï¼Œå…¶ä¸­ï¼š
- Q å¤´ï¼š8 ä¸ª
- KV å¤´ï¼š2 ä¸ªï¼ˆé€šè¿‡ `repeat_kv` é‡å¤ 4 æ¬¡åŒ¹é… Q å¤´ï¼‰

**GQA çš„ä¼˜åŠ¿**ï¼š
1. **å‡å°‘ KV Cache æ˜¾å­˜**ï¼šæ¨ç†æ—¶ KV cache å¤§å°å‡å°‘ 75%ï¼ˆä» 8 å¤´ â†’ 2 å¤´ï¼‰
2. **æå‡æ¨ç†é€Ÿåº¦**ï¼šå‡å°‘ KV æŠ•å½±çš„è®¡ç®—é‡
3. **ä¿æŒè¡¨è¾¾èƒ½åŠ›**ï¼šé€šè¿‡å…±äº« KV å¤´ï¼Œæ¨¡å‹ä»èƒ½å­¦ä¹ åˆ°ä¸°å¯Œçš„æ³¨æ„åŠ›æ¨¡å¼

### å®ç°æ­¥éª¤

#### 1.1 ä¿®æ”¹ `config.py`

åœ¨ `DIYConfig` ç±»ä¸­æ·»åŠ  `num_key_value_heads` å‚æ•°ï¼š

```python
@dataclass
class DIYConfig:
    # ... ç°æœ‰é…ç½® ...
    
    # Attention é…ç½®
    num_heads: int = 8  # Q å¤´æ•°é‡
    num_key_value_heads: int = 2  # KV å¤´æ•°é‡ï¼ˆGQAï¼‰
    
    # æ·»åŠ æ–­è¨€æ£€æŸ¥ï¼ˆå¯é€‰ï¼Œåœ¨ __post_init__ ä¸­ï¼‰
    def __post_init__(self):
        assert self.num_heads % self.num_key_value_heads == 0, \
            f"num_heads ({self.num_heads}) must be divisible by num_key_value_heads ({self.num_key_value_heads})"
```

#### 1.2 åœ¨ `attention.py` ä¸­æ·»åŠ  `repeat_kv` å‡½æ•°

åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼ˆç±»å®šä¹‰ä¹‹å‰ï¼‰æ·»åŠ ï¼š

```python
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """å°†KVå¤´é‡å¤n_repæ¬¡ä»¥åŒ¹é…Qå¤´æ•°é‡ï¼ˆGQAå®ç°ï¼‰
    
    Args:
        x: KV tensorï¼Œshape (batch_size, seq_len, num_key_value_heads, head_dim)
        n_rep: é‡å¤æ¬¡æ•°ï¼Œç­‰äº num_heads // num_key_value_heads
    
    Returns:
        é‡å¤åçš„ tensorï¼Œshape (batch_size, seq_len, num_heads, head_dim)
    """
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )
```

#### 1.3 ä¿®æ”¹ `FlashAttentionFusedAttention` ç±»

**ä¿®æ”¹ `__init__` æ–¹æ³•**ï¼š

```python
def __init__(self, config: DIYConfig):
    super().__init__()
    
    # GQA é…ç½®
    self.num_heads = config.num_heads
    self.num_key_value_heads = config.num_key_value_heads if hasattr(config, 'num_key_value_heads') else config.num_heads
    assert self.num_heads % self.num_key_value_heads == 0
    self.n_rep = self.num_heads // self.num_key_value_heads  # é‡å¤å€æ•°
    
    self.hidden_size = config.hidden_size
    self.head_dim = config.hidden_size // config.num_heads
    self.max_seq_len = config.max_seq_len
    self.dropout = config.dropout
    
    # åˆ†ç¦» Q å’Œ KV æŠ•å½±ï¼ˆGQAï¼‰
    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
    self.kv_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim * 2, bias=False)
    self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
    self.resid_dropout = nn.Dropout(self.dropout)
```

**ä¿®æ”¹ `forward` æ–¹æ³•**ï¼š

```python
def forward(self, x, ...):
    batch_size, seq_len, _ = x.shape
    
    # åˆ†ç¦»æŠ•å½± Q å’Œ KV
    q = self.q_proj(x)  # [B, L, num_heads * head_dim]
    kv = self.kv_proj(x)  # [B, L, num_key_value_heads * head_dim * 2]
    
    # æ‹†åˆ† KV
    k, v = kv.chunk(2, dim=-1)  # å„ [B, L, num_key_value_heads * head_dim]
    
    # Reshape ä¸ºå¤šå¤´æ ¼å¼
    q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
    k = k.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2).contiguous()
    v = v.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim).transpose(1, 2).contiguous()
    
    # åº”ç”¨ RoPE
    if freqs_cos is not None and freqs_sin is not None:
        from .rope import apply_rotary_pos_emb
        q, k = apply_rotary_pos_emb(q, k, freqs_cos, freqs_sin)
    
    # è®­ç»ƒæ¨¡å¼ï¼šå¤„ç† KV cache
    if self.training:
        # ... è®­ç»ƒæ¨¡å¼çš„ mask é€»è¾‘ ...
        pass
    else:
        # æ¨ç†æ¨¡å¼ï¼šå¤„ç† KV cacheï¼ˆæ³¨æ„ï¼šcache ä¸­å­˜å‚¨çš„æ˜¯æœªé‡å¤çš„ KVï¼‰
        if cached is None:
            cached = {
                "k": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim,
                               device=x.device, dtype=x.dtype),
                "v": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim,
                               device=x.device, dtype=x.dtype),
            }
            # ... åˆå§‹åŒ–é€»è¾‘ ...
        else:
            # ... æ›´æ–° cache é€»è¾‘ï¼ˆæ³¨æ„ï¼šåªæ›´æ–° num_key_value_heads ä¸ªå¤´çš„ cacheï¼‰...
            pass
        
        # âš ï¸ å…³é”®ï¼šåœ¨è®¡ç®— attention ä¹‹å‰ï¼Œå¯¹ KV åº”ç”¨ repeat_kv
        k = repeat_kv(k.transpose(1, 2), self.n_rep).transpose(1, 2)  # [B, num_heads, L, D]
        v = repeat_kv(v.transpose(1, 2), self.n_rep).transpose(1, 2)  # [B, num_heads, L, D]
    
    # è®¡ç®— attention
    att = F.scaled_dot_product_attention(q, k, v, ...)
    
    # ... åç»­å¤„ç† ...
```

**æ³¨æ„äº‹é¡¹**ï¼š
- KV cache ä¸­å­˜å‚¨çš„æ˜¯ **æœªé‡å¤çš„ KV**ï¼ˆåªæœ‰ `num_key_value_heads` ä¸ªå¤´ï¼‰
- åœ¨è®¡ç®— attention ä¹‹å‰ï¼Œå¯¹ KV åº”ç”¨ `repeat_kv` æ‰©å±•åˆ° `num_heads` ä¸ªå¤´
- è¿™æ ·å¯ä»¥å‡å°‘ cache çš„æ˜¾å­˜å ç”¨

---

## ğŸ”´ ä¼˜å…ˆçº§ 2ï¼šè°ƒæ•´æ¨¡å‹å±‚æ•°

### é—®é¢˜æè¿°

å½“å‰æ¨¡å‹æœ‰ **23 å±‚**ï¼Œè€Œ MiniMind2-Small åªæœ‰ **8 å±‚**ã€‚å±‚æ•°è¿‡å¤šä¼šå¯¼è‡´ï¼š
1. **å‚æ•°é‡è¿‡å¤§**ï¼šè¿œè¶… 26M çš„ç›®æ ‡
2. **è¿‡æ‹Ÿåˆé£é™©**ï¼šåœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´"repeater"é—®é¢˜
3. **è®­ç»ƒä¸ç¨³å®š**ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é£é™©å¢åŠ 

### å®ç°æ­¥éª¤

#### 2.1 ä¿®æ”¹ `config.py`

```python
@dataclass
class DIYConfig:
    # ... å…¶ä»–é…ç½® ...
    num_layers: int = 8  # ä» 23 æ”¹ä¸º 8ï¼ŒåŒ¹é… MiniMind2-Small
```

**å‚æ•°é‡ä¼°ç®—**ï¼ˆ8 å±‚ vs 23 å±‚ï¼‰ï¼š
- 8 å±‚ï¼šçº¦ 26M å‚æ•° âœ…
- 23 å±‚ï¼šçº¦ 75M+ å‚æ•° âŒ

---

## ğŸŸ¡ ä¼˜å…ˆçº§ 3ï¼šä¿®æ­£ FeedForward é…ç½®

### 3.1 ä¿®æ”¹æ¿€æ´»å‡½æ•°ï¼šGELU â†’ SiLU

#### é—®é¢˜æè¿°

å½“å‰ä½¿ç”¨ **GELU**ï¼Œè€Œ MiniMind2-Small ä½¿ç”¨ **SiLU**ï¼ˆSwish æ¿€æ´»å‡½æ•°ï¼‰ã€‚

**SiLU å…¬å¼**ï¼š`SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))`

#### å®ç°æ­¥éª¤

ä¿®æ”¹ `feed_forward.py` ä¸­çš„ `GatedFeedForward` ç±»ï¼š

```python
from torch.nn import SiLU  # æ·»åŠ å¯¼å…¥

class GatedFeedForward(nn.Module):
    def __init__(self, config: DIYConfig):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        
        self.linear_gate = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.linear_up = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.linear_down = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.activation = SiLU()  # ä» nn.GELU() æ”¹ä¸º SiLU()
        self.dropout = nn.Dropout(config.dropout)
```

### 3.2 ä¿®æ­£ intermediate_size è®¡ç®—æ–¹å¼

#### é—®é¢˜æè¿°

å½“å‰ `intermediate_size` ç¡¬ç¼–ç ä¸º **2048**ï¼Œè€Œ MiniMind2-Small ä½¿ç”¨è‡ªåŠ¨è®¡ç®—ï¼š
- å…¬å¼ï¼š`intermediate_size = int(hidden_size * 8 / 3)`
- ç„¶åå‘ä¸Šå–æ•´åˆ° **64 çš„å€æ•°**ï¼ˆå¯¹é½ä¼˜åŒ–ï¼‰
- å¯¹äº `hidden_size=512`ï¼š`int(512 * 8 / 3) = 1365` â†’ `64 * 22 = 1408`

#### å®ç°æ­¥éª¤

**ä¿®æ”¹ `config.py`**ï¼š

```python
@dataclass
class DIYConfig:
    # ... å…¶ä»–é…ç½® ...
    intermediate_size: Optional[int] = None  # æ”¹ä¸º Optionalï¼Œåœ¨ FeedForward ä¸­è‡ªåŠ¨è®¡ç®—
```

**ä¿®æ”¹ `feed_forward.py` ä¸­çš„ `GatedFeedForward.__init__`**ï¼š

```python
def __init__(self, config: DIYConfig):
    super().__init__()
    self.hidden_size = config.hidden_size
    
    # è‡ªåŠ¨è®¡ç®— intermediate_sizeï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if config.intermediate_size is None:
        intermediate_size = int(config.hidden_size * 8 / 3)
        # å‘ä¸Šå–æ•´åˆ° 64 çš„å€æ•°ï¼ˆå¯¹é½ä¼˜åŒ–ï¼‰
        self.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
    else:
        self.intermediate_size = config.intermediate_size
    
    self.linear_gate = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
    self.linear_up = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
    self.linear_down = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
    self.activation = SiLU()
    self.dropout = nn.Dropout(config.dropout)
```

**è®¡ç®—ç¤ºä¾‹**ï¼š
- `hidden_size=512` â†’ `intermediate_size=1408`
- `hidden_size=768` â†’ `intermediate_size=2112`ï¼ˆ64 * 33ï¼‰

---

## ğŸŸ¢ ä¼˜å…ˆçº§ 4ï¼šéªŒè¯å…¶ä»–é…ç½®

### 4.1 RoPE å®ç°éªŒè¯

å½“å‰ `rope.py` çš„å®ç°ä¸åŸå§‹é¡¹ç›®åŸºæœ¬ä¸€è‡´ï¼Œä½†éœ€è¦ç¡®è®¤ï¼š

1. **`precompute_freqs_cis`**ï¼šé¢‘ç‡è®¡ç®—æ˜¯å¦æ­£ç¡®
2. **`apply_rotary_pos_emb`**ï¼šæ—‹è½¬åº”ç”¨æ˜¯å¦æ­£ç¡®

**å‚è€ƒåŸå§‹é¡¹ç›®å®ç°**ï¼ˆ`model/model_minimind.py:131-137`ï¼‰ï¼š
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)
    
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed
```

**å½“å‰å®ç°æ£€æŸ¥**ï¼š
- âœ… `rotate_half` å®ç°æ­£ç¡®
- âš ï¸ éœ€è¦ç¡®è®¤ `unsqueeze_dim` çš„ä½¿ç”¨ï¼ˆå½“å‰å®ç°ä½¿ç”¨å›ºå®šçš„ `unsqueeze(0).unsqueeze(0)`ï¼Œåº”è¯¥ç­‰ä»·ï¼‰

### 4.2 Dropout é…ç½®

å½“å‰ `dropout=0.1`ï¼ŒåŸå§‹é¡¹ç›® `dropout=0.0`ã€‚

**å»ºè®®**ï¼šå¯ä»¥ä¿ç•™ `0.1`ï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§ï¼Œä½†ä¸ºäº†å®Œå…¨å¯¹é½å¯ä»¥æ”¹ä¸º `0.0`ã€‚

---

## ğŸ“ å®Œæ•´ä¿®æ”¹æ¸…å•

### å¿…é¡»ä¿®æ”¹ï¼ˆè§£å†³ repeater é—®é¢˜ï¼‰

1. âœ… **å®ç° GQA**
   - [ ] åœ¨ `config.py` æ·»åŠ  `num_key_value_heads=2`
   - [ ] åœ¨ `attention.py` æ·»åŠ  `repeat_kv` å‡½æ•°
   - [ ] ä¿®æ”¹ `FlashAttentionFusedAttention` æ”¯æŒ GQA
   - [ ] ä¿®æ”¹ KV cache é€»è¾‘ï¼ˆå­˜å‚¨æœªé‡å¤çš„ KVï¼‰

2. âœ… **è°ƒæ•´å±‚æ•°**
   - [ ] `config.py`: `num_layers` ä» 23 â†’ 8

3. âœ… **ä¿®æ­£ FeedForward**
   - [ ] `feed_forward.py`: GELU â†’ SiLU
   - [ ] `config.py`: `intermediate_size` æ”¹ä¸º `Optional[int] = None`
   - [ ] `feed_forward.py`: æ·»åŠ è‡ªåŠ¨è®¡ç®—é€»è¾‘ï¼ˆ8/3 å€ï¼Œå‘ä¸Šå–æ•´åˆ° 64ï¼‰

### å¯é€‰ä¿®æ”¹ï¼ˆå®Œå…¨å¯¹é½ï¼‰

4. âš ï¸ **Dropout**
   - [ ] `config.py`: `dropout` ä» 0.1 â†’ 0.0ï¼ˆå¯é€‰ï¼‰

5. âš ï¸ **éªŒè¯ RoPE**
   - [ ] æ£€æŸ¥ `rope.py` å®ç°æ˜¯å¦å®Œå…¨ä¸€è‡´

---

## ğŸ¯ ä¼˜åŒ–åçš„é¢„æœŸæ•ˆæœ

å®Œæˆä»¥ä¸Šä¼˜åŒ–åï¼Œæ¨¡å‹é…ç½®å°†å®Œå…¨å¯¹é½ MiniMind2-Smallï¼š

| é…ç½®é¡¹ | ä¼˜åŒ–åå€¼ |
|--------|----------|
| å‚æ•°é‡ | ~26M âœ… |
| GQA | âœ… å·²å®ç° |
| å±‚æ•° | 8 âœ… |
| æ¿€æ´»å‡½æ•° | SiLU âœ… |
| intermediate_size | 1408ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰âœ… |

**é¢„æœŸæ”¹å–„**ï¼š
1. âœ… è§£å†³"repeater"é—®é¢˜ï¼ˆå±‚æ•°å‡å°‘ + GQAï¼‰
2. âœ… å‚æ•°é‡å¯¹é½ 26M
3. âœ… æ¨ç†æ—¶ KV cache æ˜¾å­˜å‡å°‘ 75%
4. âœ… è®­ç»ƒæ›´ç¨³å®šï¼ˆå±‚æ•°å‡å°‘ï¼‰

---

## ğŸ” éªŒè¯æ­¥éª¤

å®Œæˆä¿®æ”¹åï¼Œå»ºè®®è¿›è¡Œä»¥ä¸‹éªŒè¯ï¼š

1. **å‚æ•°é‡éªŒè¯**ï¼š
   ```python
   from llm_study.model import DIYForCausalLM
   from llm_study.config import DIYConfig
   
   config = DIYConfig()
   model = DIYForCausalLM(config)
   total_params = sum(p.numel() for p in model.parameters())
   print(f"Total parameters: {total_params / 1e6:.2f}M")  # åº”è¯¥æ¥è¿‘ 26M
   ```

2. **GQA éªŒè¯**ï¼š
   - æ£€æŸ¥ KV cache å¤§å°æ˜¯å¦ä¸º Q cache çš„ 1/4
   - éªŒè¯ `repeat_kv` å‡½æ•°æ­£ç¡®å·¥ä½œ

3. **è®­ç»ƒéªŒè¯**ï¼š
   - é‡æ–°é¢„è®­ç»ƒæ¨¡å‹
   - æ£€æŸ¥ loss æ›²çº¿æ˜¯å¦æ­£å¸¸ä¸‹é™
   - æ¨ç†æµ‹è¯•æ˜¯å¦è¿˜æœ‰"repeater"é—®é¢˜

---

## ğŸ“š å‚è€ƒèµ„æ–™

- MiniMind åŸå§‹é¡¹ç›®ï¼š`./model/model_minimind.py`
- README.mdï¼šæ¨¡å‹å‚æ•°é…ç½®è¡¨
- GQA è®ºæ–‡ï¼šGrouped Query Attention (GQA) ç›¸å…³ç ”ç©¶

---

**æœ€åæ›´æ–°**ï¼š2026-02-08
